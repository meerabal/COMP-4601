Meera Balsara
101152760

Assignment 1

Instance details for OpenStack: 
Name: MeeraB. 4601
IP: 192.168.72.93, 134.117.133.66

Video link:
https://carleton-ca.zoom.us/rec/share/NM4OhwyxnpYHNIKwm8AHN442MN7hUMC5sXWOWsdXkfsGEoJB_GhJDel257MhwA9V.ituXeDrbugg1Dkty?startTime=1667789679000
Passcode: mXp**T&8

1. The crawler extracts content, links from the pages. It stores the data as maps until it is processed and added to the database. Yes, there is some intermediary processing which is performed by running the pagerank code which calculated the pagerank for each page and updates the entries in the database.

2. The server supports GET requests for getting the index (which contains links to the two main pages), getting the fruits and personal website search pages.
Each search is performed through a search query being passed to the route. Eg:
/personal?q=ring&boost=true&limit=20
is supported through the GET request to the /personal route with query q containing the search string, boost containing a boolean whether the score should be boosted by pagerank, and the limit which specifies number of results to be served.
Pages for each result are also supported through GET requests of the form:
/personal/:id 
-- where id is the id of the result entry / document
similarly for fruits:
/fruits/:id 
This highlights the RESTful design of the server.

3. Content score is generated by using the score from elasticlunr's calculations which uses combined boolean model, TF/IDF model and the vector space model. Since I have set the bool variable to "AND", it checks all the fields and considers the documents which have all the terms in the search query before applying the TF/IDF and vector space model. When the boost parameter is set to true, content score is boosted by pagerank.

content score = (elasticlunr index search score) * (pagerank)

4. I used the order of the array returned after querying the database. Firstly I check which links are at which array positions. After that, I create a 2D array with the outgoing links for each page. Each row will contain an array of 1s and 0s. So if the array has N-7 at position 0 and N-0 at position 5, and N-7 has an outgoing link to N-0, the 5th column in row 0 will be set to 1. This logic is similar to the one I implemented for lab 5.

5. The page selection policy for the crawler for my personal site is based on breadth-first search and pagerank. It is also sorted dependent on the content score generated by the calculation mentioned above.

6. I selected the Lord of the Rings wiki because it seemed like a challenging one to do and Dave loves LOTR so I figured why not. I have not watched LOTR myself. 
I ran into issues while thinking of which content I wanted to cover and which pages I wanted to skip. I also had to weed out external and reference links. I did this by adding if conditions to check for the links and performing string manipulation to clean some of the data. I also ended up picking text only from a certain div child in the page which took me quite a while to navigate since I do not have any knowledge in jQuery. So I fought with cheerio to figure out how that works. The links within the wiki are relative so I performed some string manipulation with the before placing it in the queue.

7. Honestly, I feel like there is a lot of room for improvement. The method of crawling I have used is not the most efficient. I think indexing the data also could have been handled in a much more efficient way. The page wiki contains a lot of other data which could be indexed as well and it could prove useful for a bigger project. There are useless words in the content stored which provide no real value to the search engine, so it would be great if they were omitted / weeded out. In terms of scalability, this solution will not scale well by itself but with the improvements mentioned above, I believe that it should provide a much smoother experience.